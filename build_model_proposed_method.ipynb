{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flwr.common import (EvaluateRes, FitRes, NDArrays, Parameters, Scalar,\n",
    "                         ndarrays_to_parameters, parameters_to_ndarrays)\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import weighted_loss_avg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultipleLinearRegressionModel, self).__init__()\n",
    "        self.hidden1 = nn.Linear(100, 50)  \n",
    "        self.dropout1 = nn.Dropout(p=0.2) \n",
    "        self.hidden2 = nn.Linear(50, 10)   \n",
    "        self.dropout2 = nn.Dropout(p=0.2) \n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def train(model,train_loader,criterion,optimizer,epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for X_train_tensor,y_train_tensor in train_loader:\n",
    "            X_train_tensor,y_train_tensor = X_train_tensor.to(device),y_train_tensor.to(device)\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test(model,val_loader,criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_tensor,y_val_tensor in val_loader:\n",
    "            X_val_tensor,y_val_tensor = X_val_tensor.to(device),y_val_tensor.to(device)\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss += criterion(val_outputs, y_val_tensor).item()\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    return average_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate pairwise difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_difference_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    class_counts = df[\"grade\"].value_counts()\n",
    "    valid_classes = class_counts[class_counts > 1].index\n",
    "    df_filtered = df[df[\"grade\"].isin(valid_classes)]\n",
    "\n",
    "    a_df, b_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered[\"grade\"], random_state=42)\n",
    "    \n",
    "    df2 = pd.DataFrame()\n",
    "    df3 = pd.DataFrame()\n",
    "    \n",
    "    numeric_a_df = a_df.select_dtypes(include=[int, float])\n",
    "    for i in range(len(numeric_a_df)):\n",
    "        for j in range(len(numeric_a_df)):\n",
    "            if i != j:\n",
    "                diff = numeric_a_df.iloc[j] - numeric_a_df.iloc[i]\n",
    "                diff = pd.concat([diff, pd.Series([a_df[\"userid\"].iloc[i], a_df[\"userid\"].iloc[j]], index=['user_1', 'user_2'])])\n",
    "                df2 = pd.concat([df2, diff.to_frame().T], ignore_index=True)\n",
    "\n",
    "    numeric_b_df = b_df.select_dtypes(include=[int, float])\n",
    "    for i in range(len(numeric_b_df)):\n",
    "        for j in range(len(numeric_b_df)):\n",
    "            if i != j:\n",
    "                diff2 = numeric_b_df.iloc[j] - numeric_b_df.iloc[i]\n",
    "                diff2 = pd.concat([diff2, pd.Series([b_df[\"userid\"].iloc[i], b_df[\"userid\"].iloc[j]], index=['user_1', 'user_2'])])\n",
    "                df3 = pd.concat([df3, diff2.to_frame().T], ignore_index=True)\n",
    "    return df2, df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to prepare training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    file_paths = [\n",
    "        \"./learnfd_data/CourseA-2019_100dim.csv\",\n",
    "        \"./learnfd_data/CourseA-2020_100dim.csv\",\n",
    "        \"./learnfd_data/CourseA-2021_100dim.csv\",\n",
    "        \"./learnfd_data/CourseB-2019_100dim.csv\",\n",
    "        \"./learnfd_data/CourseC-2021-1_100dim.csv\",\n",
    "        \"./learnfd_data/CourseC-2021-2_100dim.csv\",\n",
    "        \"./learnfd_data/CourseD-2020_100dim.csv\",\n",
    "        \"./learnfd_data/CourseD-2021_100dim.csv\",\n",
    "        \"./learnfd_data/CourseE-2020-1_100dim.csv\",\n",
    "        \"./learnfd_data/CourseE-2020-2_100dim.csv\",\n",
    "        \"./learnfd_data/CourseF-2021_100dim.csv\",\n",
    "        \"./learnfd_data/CourseG-2021_100dim.csv\"\n",
    "    ]\n",
    "    num_client = 0\n",
    "    train_loaders = [] \n",
    "    val_loaders = [] \n",
    "\n",
    "    for file_path in file_paths:\n",
    "        num_client += 1\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        train_df, val_df = make_difference_data(file_path)\n",
    "        \n",
    "\n",
    "        feature_columns = [str(i) for i in range(100)]\n",
    "        X_train = train_df[feature_columns].apply(pd.to_numeric, errors='coerce').values\n",
    "        y_train = train_df['grade'].apply(pd.to_numeric, errors='coerce').values.reshape(-1, 1)\n",
    "        X_val = val_df[feature_columns].apply(pd.to_numeric, errors='coerce').values\n",
    "        y_val = val_df['grade'].apply(pd.to_numeric, errors='coerce').values.reshape(-1, 1) \n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        batch_size = 32 \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        train_loaders.append(train_loader)\n",
    "        val_loaders.append(val_loader)\n",
    "\n",
    "    return train_loaders, val_loaders, num_client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Flower client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, train_loader, val_loader) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader= val_loader\n",
    "        self.model = MultipleLinearRegressionModel().to(self.device)\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.0005)\n",
    "\n",
    "        train(self.model, self.train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "        return self.get_parameters({}), len(self.train_loader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = test(self.model, self.val_loader, criterion=criterion)\n",
    "        \n",
    "        return float(loss), len(self.val_loader), {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the strategy for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateCustomMetricStrategy(fl.server.strategy.FedAvg):  \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss=float(\"inf\")\n",
    "        self.path_bool = False\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        weights_results = [\n",
    "                (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "                for _, fit_res in results\n",
    "            ]\n",
    "\n",
    "        num_examples_total = sum(num_examples for (_, num_examples) in weights_results)\n",
    "\n",
    "        weighted_weights = [\n",
    "            [layer * num_examples for layer in weights] for weights, num_examples in weights_results\n",
    "        ]\n",
    "\n",
    "        weights_prime: NDArrays = [\n",
    "            reduce(np.add, layer_updates) / num_examples_total\n",
    "            for layer_updates in zip(*weighted_weights)\n",
    "        ]\n",
    "        aggregated_parameters = ndarrays_to_parameters(weights_prime)\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        \n",
    "        if aggregated_parameters is not None:\n",
    "            if self.path_bool:\n",
    "                aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "                params_dict = zip(best_net.state_dict().keys(), aggregated_ndarrays)\n",
    "                state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "                best_net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        return aggregated_parameters,metrics_aggregated\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \n",
    "        if not results:\n",
    "            return None, {}\n",
    "        if not self.accept_failures and failures:\n",
    "            return None, {}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if(loss_aggregated<self.loss):\n",
    "            self.loss= loss_aggregated\n",
    "            self.path_bool=True\n",
    "        else:\n",
    "            self.path_bool=False\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        if self.evaluate_metrics_aggregation_fn:\n",
    "            eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n",
    "            metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n",
    "\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "    \n",
    "def get_params(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate client instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn(train_loader,val_loader):\n",
    "    def client_fn(cid: str):\n",
    "\n",
    "        return FlowerClient(\n",
    "           train_loader=train_loader[int(cid)], val_loader=val_loader[int(cid)]\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders,val_loaders, num_client=prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run federated learning simulation and save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for times in range(1,11):\n",
    "    best_net = MultipleLinearRegressionModel() \n",
    "    params=get_params(best_net)\n",
    "\n",
    "    strategy = AggregateCustomMetricStrategy(\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "        fraction_fit=1,  \n",
    "        fraction_evaluate=1,  \n",
    "        min_available_clients=num_client, \n",
    "    )  \n",
    "\n",
    "    client_fn_callback = generate_client_fn(train_loaders,val_loaders)\n",
    "    my_client_resources = {\"num_cpus\": 2, \"num_gpus\": 0.16}\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn_callback, \n",
    "        num_clients=num_client, \n",
    "        config=fl.server.ServerConfig(num_rounds=250),  \n",
    "        strategy=strategy, \n",
    "        client_resources = my_client_resources\n",
    "    )\n",
    "    save_dir = r\"./pth_register\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    torch.save(best_net.state_dict(), os.path.join(save_dir, f\"test2_best_model_proposed_method_{times}.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
